{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Language Modeling.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOQZTtfmgATGUAsApGdZs8V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Adeelzafar/NLP-Course/blob/main/Language_Modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENW9cWUJRl4N",
        "outputId": "98289e31-002a-4761-a50a-7cf7739002fa"
      },
      "source": [
        "#N-grams\n",
        "import nltk               # NLP toolkit\n",
        "import re                 # Library for Regular expression operations\n",
        "\n",
        "nltk.download('punkt')    # Download the Punkt sentence tokenizer \n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LYRytC5NRskg",
        "outputId": "bb292617-b72c-4714-bb12-b38f3b162649"
      },
      "source": [
        "# change the corpus to lowercase\n",
        "corpus = \"Learning% makes 'me' happy. I am happy be-cause I am learning! :)\"\n",
        "corpus = corpus.lower()\n",
        "\n",
        "# note that word \"learning\" will now be the same regardless of its position in the sentence\n",
        "print(corpus)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "learning% makes 'me' happy. i am happy be-cause i am learning! :)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MThwzL35Rv4G",
        "outputId": "784b66ca-ee6e-4ebc-948d-0e2970dd1b3a"
      },
      "source": [
        "# remove special characters\n",
        "corpus = \"learning% makes 'me' happy. i am happy be-cause i am learning! :)\"\n",
        "corpus = re.sub(r\"[^a-zA-Z0-9.?! ]+\", \"\", corpus)\n",
        "print(corpus)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "learning makes me happy. i am happy because i am learning! \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0lBAvsQRyV9",
        "outputId": "d5d2ff2b-d0e7-48c0-bf66-4d00dce74b59"
      },
      "source": [
        "# split text by a delimiter to array\n",
        "input_date=\"Sat May  9 07:33:35 CEST 2020\"\n",
        "\n",
        "# get the date parts in array\n",
        "date_parts = input_date.split(\" \")\n",
        "print(f\"date parts = {date_parts}\")\n",
        "\n",
        "#get the time parts in array\n",
        "time_parts = date_parts[4].split(\":\")\n",
        "print(f\"time parts = {time_parts}\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "date parts = ['Sat', 'May', '', '9', '07:33:35', 'CEST', '2020']\n",
            "time parts = ['07', '33', '35']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOghiOnYR2Vx",
        "outputId": "0197383a-c283-4585-86ec-55bdde9f0613"
      },
      "source": [
        "# tokenize the sentence into an array of words\n",
        "\n",
        "sentence = 'i am happy because i am learning.'\n",
        "tokenized_sentence = nltk.word_tokenize(sentence)\n",
        "print(f'{sentence} -> {tokenized_sentence}')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i am happy because i am learning. -> ['i', 'am', 'happy', 'because', 'i', 'am', 'learning', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUCrys6BR6DH",
        "outputId": "419ac519-cefa-4ef0-cdd5-3c70fe8fe042"
      },
      "source": [
        "# find length of each word in the tokenized sentence\n",
        "sentence = ['i', 'am', 'happy', 'because', 'i', 'am', 'learning', '.']\n",
        "word_lengths = [(word, len(word)) for word in sentence] # Create a list with the word lengths using a list comprehension\n",
        "print(f' Lengths of the words: \\n{word_lengths}')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Lengths of the words: \n",
            "[('i', 1), ('am', 2), ('happy', 5), ('because', 7), ('i', 1), ('am', 2), ('learning', 8), ('.', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQIPbdy4R9kf",
        "outputId": "225f762a-504f-4afe-cf16-a9fff020c5e6"
      },
      "source": [
        "def sentence_to_trigram(tokenized_sentence):\n",
        "    \"\"\"\n",
        "    Prints all trigrams in the given tokenized sentence.\n",
        "    \n",
        "    Args:\n",
        "        tokenized_sentence: The words list.\n",
        "    \n",
        "    Returns:\n",
        "        No output\n",
        "    \"\"\"\n",
        "    # note that the last position of i is 3rd to the end\n",
        "    for i in range(len(tokenized_sentence) - 3 + 1):\n",
        "        # the sliding window starts at position i and contains 3 words\n",
        "        trigram = tokenized_sentence[i : i + 3]\n",
        "        print(trigram)\n",
        "\n",
        "tokenized_sentence = ['i', 'am', 'happy', 'because', 'i', 'am', 'learning', '.']\n",
        "\n",
        "print(f'List all trigrams of sentence: {tokenized_sentence}\\n')\n",
        "sentence_to_trigram(tokenized_sentence)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "List all trigrams of sentence: ['i', 'am', 'happy', 'because', 'i', 'am', 'learning', '.']\n",
            "\n",
            "['i', 'am', 'happy']\n",
            "['am', 'happy', 'because']\n",
            "['happy', 'because', 'i']\n",
            "['because', 'i', 'am']\n",
            "['i', 'am', 'learning']\n",
            "['am', 'learning', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d20R75wqSDLf",
        "outputId": "f462d85d-ce34-4b9e-b961-19cb705b3baf"
      },
      "source": [
        "# get trigram prefix from a 4-gram\n",
        "fourgram = ['i', 'am', 'happy','because']\n",
        "trigram = fourgram[0:-1] # Get the elements from 0, included, up to the last element, not included.\n",
        "print(trigram)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'am', 'happy']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zj5QRMEsSHRG",
        "outputId": "4ba69282-7c64-4dfb-a0c2-69922686a7e9"
      },
      "source": [
        "# when working with trigrams, you need to prepend 2 <s> and append one </s>\n",
        "n = 3\n",
        "tokenized_sentence = ['i', 'am', 'happy', 'because', 'i', 'am', 'learning', '.']\n",
        "tokenized_sentence = [\"<s>\"] * (n - 1) + tokenized_sentence + [\"<e>\"]\n",
        "print(tokenized_sentence)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<s>', '<s>', 'i', 'am', 'happy', 'because', 'i', 'am', 'learning', '.', '<e>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77r2yjXFSKMW",
        "outputId": "b58469c2-ea53-4e5e-fa55-0af8adfc9334"
      },
      "source": [
        "# manipulate n_gram count dictionary\n",
        "\n",
        "n_gram_counts = {\n",
        "    ('i', 'am', 'happy'): 2,\n",
        "    ('am', 'happy', 'because'): 1}\n",
        "\n",
        "# get count for an n-gram tuple\n",
        "print(f\"count of n-gram {('i', 'am', 'happy')}: {n_gram_counts[('i', 'am', 'happy')]}\")\n",
        "\n",
        "# check if n-gram is present in the dictionary\n",
        "if ('i', 'am', 'learning') in n_gram_counts:\n",
        "    print(f\"n-gram {('i', 'am', 'learning')} found\")\n",
        "else:\n",
        "    print(f\"n-gram {('i', 'am', 'learning')} missing\")\n",
        "\n",
        "# update the count in the word count dictionary\n",
        "n_gram_counts[('i', 'am', 'learning')] = 1\n",
        "if ('i', 'am', 'learning') in n_gram_counts:\n",
        "    print(f\"n-gram {('i', 'am', 'learning')} found\")\n",
        "else:\n",
        "    print(f\"n-gram {('i', 'am', 'learning')} missing\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "count of n-gram ('i', 'am', 'happy'): 2\n",
            "n-gram ('i', 'am', 'learning') missing\n",
            "n-gram ('i', 'am', 'learning') found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vag6oGJcSc6q",
        "outputId": "47a43d15-73b6-48be-ff3e-2688b0162082"
      },
      "source": [
        "# concatenate tuple for prefix and tuple with the last word to create the n_gram\n",
        "prefix = ('i', 'am', 'happy')\n",
        "word = 'because'\n",
        "\n",
        "# note here the syntax for creating a tuple for a single word\n",
        "n_gram = prefix + (word,)\n",
        "print(n_gram)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('i', 'am', 'happy', 'because')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGtnQ0B9ShDZ",
        "outputId": "919e5b3f-6d11-41a2-f46e-9e42067982be"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "def single_pass_trigram_count_matrix(corpus):\n",
        "    \"\"\"\n",
        "    Creates the trigram count matrix from the input corpus in a single pass through the corpus.\n",
        "    \n",
        "    Args:\n",
        "        corpus: Pre-processed and tokenized corpus. \n",
        "    \n",
        "    Returns:\n",
        "        bigrams: list of all bigram prefixes, row index\n",
        "        vocabulary: list of all found words, the column index\n",
        "        count_matrix: pandas dataframe with bigram prefixes as rows, \n",
        "                      vocabulary words as columns \n",
        "                      and the counts of the bigram/word combinations (i.e. trigrams) as values\n",
        "    \"\"\"\n",
        "    bigrams = []\n",
        "    vocabulary = []\n",
        "    count_matrix_dict = defaultdict(dict)\n",
        "    \n",
        "    # go through the corpus once with a sliding window\n",
        "    for i in range(len(corpus) - 3 + 1):\n",
        "        # the sliding window starts at position i and contains 3 words\n",
        "        trigram = tuple(corpus[i : i + 3])\n",
        "        \n",
        "        bigram = trigram[0 : -1]\n",
        "        if not bigram in bigrams:\n",
        "            bigrams.append(bigram)        \n",
        "        \n",
        "        last_word = trigram[-1]\n",
        "        if not last_word in vocabulary:\n",
        "            vocabulary.append(last_word)\n",
        "        \n",
        "        if (bigram,last_word) not in count_matrix_dict:\n",
        "            count_matrix_dict[bigram,last_word] = 0\n",
        "            \n",
        "        count_matrix_dict[bigram,last_word] += 1\n",
        "    \n",
        "    # convert the count_matrix to np.array to fill in the blanks\n",
        "    count_matrix = np.zeros((len(bigrams), len(vocabulary)))\n",
        "    for trigram_key, trigam_count in count_matrix_dict.items():\n",
        "        count_matrix[bigrams.index(trigram_key[0]), \\\n",
        "                     vocabulary.index(trigram_key[1])]\\\n",
        "        = trigam_count\n",
        "    \n",
        "    # np.array to pandas dataframe conversion\n",
        "    count_matrix = pd.DataFrame(count_matrix, index=bigrams, columns=vocabulary)\n",
        "    return bigrams, vocabulary, count_matrix\n",
        "\n",
        "corpus = ['i', 'am', 'happy', 'because', 'i', 'am', 'learning', '.']\n",
        "\n",
        "bigrams, vocabulary, count_matrix = single_pass_trigram_count_matrix(corpus)\n",
        "\n",
        "print(count_matrix)\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                  happy  because    i   am  learning    .\n",
            "(i, am)             1.0      0.0  0.0  0.0       1.0  0.0\n",
            "(am, happy)         0.0      1.0  0.0  0.0       0.0  0.0\n",
            "(happy, because)    0.0      0.0  1.0  0.0       0.0  0.0\n",
            "(because, i)        0.0      0.0  0.0  1.0       0.0  0.0\n",
            "(am, learning)      0.0      0.0  0.0  0.0       0.0  1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_ayynRBSpAY",
        "outputId": "0ce09708-f23d-4442-e860-89a0ba2e783b"
      },
      "source": [
        "# create the probability matrix from the count matrix\n",
        "row_sums = count_matrix.sum(axis=1)\n",
        "# divide each row by its sum\n",
        "prob_matrix = count_matrix.div(row_sums, axis=0)\n",
        "\n",
        "print(prob_matrix)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                  happy  because    i   am  learning    .\n",
            "(i, am)             0.5      0.0  0.0  0.0       0.5  0.0\n",
            "(am, happy)         0.0      1.0  0.0  0.0       0.0  0.0\n",
            "(happy, because)    0.0      0.0  1.0  0.0       0.0  0.0\n",
            "(because, i)        0.0      0.0  0.0  1.0       0.0  0.0\n",
            "(am, learning)      0.0      0.0  0.0  0.0       0.0  1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6jyAQrzDStY0",
        "outputId": "fc1eaf83-60fb-472c-94d2-bc2e0c573fce"
      },
      "source": [
        "# find the probability of a trigram in the probability matrix\n",
        "trigram = ('i', 'am', 'happy')\n",
        "\n",
        "# find the prefix bigram \n",
        "bigram = trigram[:-1]\n",
        "print(f'bigram: {bigram}')\n",
        "\n",
        "# find the last word of the trigram\n",
        "word = trigram[-1]\n",
        "print(f'word: {word}')\n",
        "\n",
        "# we are using the pandas dataframes here, column with vocabulary word comes first, row with the prefix bigram second\n",
        "trigram_probability = prob_matrix[word][bigram]\n",
        "print(f'trigram_probability: {trigram_probability}')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bigram: ('i', 'am')\n",
            "word: happy\n",
            "trigram_probability: 0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hFoUCrLsSx8_",
        "outputId": "e6a6a9ba-bb33-482e-ed1e-afe4280f0806"
      },
      "source": [
        "# lists all words in vocabulary starting with a given prefix\n",
        "vocabulary = ['i', 'am', 'happy', 'because', 'learning', '.', 'have', 'you', 'seen','it', '?']\n",
        "starts_with = 'ha'\n",
        "\n",
        "print(f'words in vocabulary starting with prefix: {starts_with}\\n')\n",
        "for word in vocabulary:\n",
        "    if word.startswith(starts_with):\n",
        "        print(word)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "words in vocabulary starting with prefix: ha\n",
            "\n",
            "happy\n",
            "have\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-3yPUjQS1Xs",
        "outputId": "ec98cef3-e23d-4be6-9775-45e21fe0ef8f"
      },
      "source": [
        "# we only need train and validation %, test is the remainder\n",
        "import random\n",
        "def train_validation_test_split(data, train_percent, validation_percent):\n",
        "    \"\"\"\n",
        "    Splits the input data to  train/validation/test according to the percentage provided\n",
        "    \n",
        "    Args:\n",
        "        data: Pre-processed and tokenized corpus, i.e. list of sentences.\n",
        "        train_percent: integer 0-100, defines the portion of input corpus allocated for training\n",
        "        validation_percent: integer 0-100, defines the portion of input corpus allocated for validation\n",
        "        \n",
        "        Note: train_percent + validation_percent need to be <=100\n",
        "              the reminder to 100 is allocated for the test set\n",
        "    \n",
        "    Returns:\n",
        "        train_data: list of sentences, the training part of the corpus\n",
        "        validation_data: list of sentences, the validation part of the corpus\n",
        "        test_data: list of sentences, the test part of the corpus\n",
        "    \"\"\"\n",
        "    # fixed seed here for reproducibility\n",
        "    random.seed(87)\n",
        "    \n",
        "    # reshuffle all input sentences\n",
        "    random.shuffle(data)\n",
        "\n",
        "    train_size = int(len(data) * train_percent / 100)\n",
        "    train_data = data[0:train_size]\n",
        "    \n",
        "    validation_size = int(len(data) * validation_percent / 100)\n",
        "    validation_data = data[train_size:train_size + validation_size]\n",
        "    \n",
        "    test_data = data[train_size + validation_size:]\n",
        "    \n",
        "    return train_data, validation_data, test_data\n",
        "\n",
        "data = [x for x in range (0, 100)]\n",
        "\n",
        "train_data, validation_data, test_data = train_validation_test_split(data, 80, 10)\n",
        "print(\"split 80/10/10:\\n\",f\"train data:{train_data}\\n\", f\"validation data:{validation_data}\\n\", \n",
        "      f\"test data:{test_data}\\n\")\n",
        "\n",
        "train_data, validation_data, test_data = train_validation_test_split(data, 98, 1)\n",
        "print(\"split 98/1/1:\\n\",f\"train data:{train_data}\\n\", f\"validation data:{validation_data}\\n\", \n",
        "      f\"test data:{test_data}\\n\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "split 80/10/10:\n",
            " train data:[28, 76, 5, 0, 62, 29, 54, 95, 88, 58, 4, 22, 92, 14, 50, 77, 47, 33, 75, 68, 56, 74, 43, 80, 83, 84, 73, 93, 66, 87, 9, 91, 64, 79, 20, 51, 17, 27, 12, 31, 67, 81, 7, 34, 45, 72, 38, 30, 16, 60, 40, 86, 48, 21, 70, 59, 6, 19, 2, 99, 37, 36, 52, 61, 97, 44, 26, 57, 89, 55, 53, 85, 3, 39, 10, 71, 23, 32, 25, 8]\n",
            " validation data:[78, 65, 63, 11, 49, 98, 1, 46, 15, 41]\n",
            " test data:[90, 96, 82, 42, 35, 13, 69, 24, 94, 18]\n",
            "\n",
            "split 98/1/1:\n",
            " train data:[66, 23, 29, 28, 52, 87, 70, 13, 15, 2, 62, 43, 82, 50, 40, 32, 30, 79, 71, 89, 6, 10, 34, 78, 11, 49, 39, 42, 26, 46, 58, 96, 97, 8, 56, 86, 33, 93, 92, 91, 57, 65, 95, 20, 72, 3, 12, 9, 47, 37, 67, 1, 16, 74, 53, 99, 54, 68, 5, 18, 27, 17, 48, 36, 24, 45, 73, 19, 41, 59, 21, 98, 0, 31, 4, 85, 80, 64, 84, 88, 25, 44, 61, 22, 60, 94, 76, 38, 77, 81, 90, 69, 63, 7, 51, 14, 55, 83]\n",
            " validation data:[35]\n",
            " test data:[75]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YrzG926TS6ey",
        "outputId": "e603221e-4220-4698-c7e1-911c06677fff"
      },
      "source": [
        "# to calculate the exponent, use the following syntax\n",
        "p = 10 ** (-250)\n",
        "M = 100\n",
        "perplexity = p ** (-1 / M)\n",
        "print(perplexity)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "316.22776601683796\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2QyYwXzTBRk",
        "outputId": "c27c8d5d-b4ac-4a53-eae7-e87fc29ca334"
      },
      "source": [
        "# build the vocabulary from M most frequent words\n",
        "# use Counter object from the collections library to find M most common words\n",
        "from collections import Counter\n",
        "\n",
        "# the target size of the vocabulary\n",
        "M = 3\n",
        "\n",
        "# pre-calculated word counts\n",
        "# Counter could be used to build this dictionary from the source corpus\n",
        "word_counts = {'happy': 5, 'because': 3, 'i': 2, 'am': 2, 'learning': 3, '.': 1}\n",
        "\n",
        "vocabulary = Counter(word_counts).most_common(M)\n",
        "\n",
        "# remove the frequencies and leave just the words\n",
        "vocabulary = [w[0] for w in vocabulary]\n",
        "\n",
        "print(f\"the new vocabulary containing {M} most frequent words: {vocabulary}\\n\") "
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the new vocabulary containing 3 most frequent words: ['happy', 'because', 'learning']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4e4O3lr6Tw3g",
        "outputId": "6978392e-87fe-4a5a-93f1-262aba5a9b10"
      },
      "source": [
        "# test if words in the input sentences are in the vocabulary, if OOV, print <UNK>\n",
        "sentence = ['am', 'i', 'learning']\n",
        "output_sentence = []\n",
        "print(f\"input sentence: {sentence}\")\n",
        "\n",
        "for w in sentence:\n",
        "    # test if word w is in vocabulary\n",
        "    if w in vocabulary:\n",
        "        output_sentence.append(w)\n",
        "    else:\n",
        "        output_sentence.append('<UNK>')\n",
        "        \n",
        "print(f\"output sentence: {output_sentence}\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input sentence: ['am', 'i', 'learning']\n",
            "output sentence: ['<UNK>', '<UNK>', 'learning']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YM6pjABJT0ue",
        "outputId": "10bef0a8-bade-4c4c-a043-f358dc4abab2"
      },
      "source": [
        "# iterate through all word counts and print words with given frequency f\n",
        "f = 3\n",
        "\n",
        "word_counts = {'happy': 5, 'because': 3, 'i': 2, 'am': 2, 'learning':3, '.': 1}\n",
        "\n",
        "for word, freq in word_counts.items():\n",
        "    if freq == f:\n",
        "        print(word)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "because\n",
            "learning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3l809IstT3yW",
        "outputId": "5c767ced-ba17-440d-bda8-9a18f63b9b5d"
      },
      "source": [
        "# many <unk> low perplexity \n",
        "training_set = ['i', 'am', 'happy', 'because','i', 'am', 'learning', '.']\n",
        "training_set_unk = ['i', 'am', '<UNK>', '<UNK>','i', 'am', '<UNK>', '<UNK>']\n",
        "\n",
        "test_set = ['i', 'am', 'learning']\n",
        "test_set_unk = ['i', 'am', '<UNK>']\n",
        "\n",
        "M = len(test_set)\n",
        "probability = 1\n",
        "probability_unk = 1\n",
        "\n",
        "# pre-calculated probabilities\n",
        "bigram_probabilities = {('i', 'am'): 1.0, ('am', 'happy'): 0.5, ('happy', 'because'): 1.0, ('because', 'i'): 1.0, ('am', 'learning'): 0.5, ('learning', '.'): 1.0}\n",
        "bigram_probabilities_unk = {('i', 'am'): 1.0, ('am', '<UNK>'): 1.0, ('<UNK>', '<UNK>'): 0.5, ('<UNK>', 'i'): 0.25}\n",
        "\n",
        "# got through the test set and calculate its bigram probability\n",
        "for i in range(len(test_set) - 2 + 1):\n",
        "    bigram = tuple(test_set[i: i + 2])\n",
        "    probability = probability * bigram_probabilities[bigram]\n",
        "        \n",
        "    bigram_unk = tuple(test_set_unk[i: i + 2])\n",
        "    probability_unk = probability_unk * bigram_probabilities_unk[bigram_unk]\n",
        "\n",
        "# calculate perplexity for both original test set and test set with <UNK>\n",
        "perplexity = probability ** (-1 / M)\n",
        "perplexity_unk = probability_unk ** (-1 / M)\n",
        "\n",
        "print(f\"perplexity for the training set: {perplexity}\")\n",
        "print(f\"perplexity for the training set with <UNK>: {perplexity_unk}\")\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "perplexity for the training set: 1.2599210498948732\n",
            "perplexity for the training set with <UNK>: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CxxF45MhT80S",
        "outputId": "065bbd21-74da-4830-bd3b-17f9f9dd7c3e"
      },
      "source": [
        "def add_k_smooting_probability(k, vocabulary_size, n_gram_count, n_gram_prefix_count):\n",
        "    numerator = n_gram_count + k\n",
        "    denominator = n_gram_prefix_count + k * vocabulary_size\n",
        "    return numerator / denominator\n",
        "\n",
        "trigram_probabilities = {('i', 'am', 'happy') : 2}\n",
        "bigram_probabilities = {( 'am', 'happy') : 10}\n",
        "vocabulary_size = 5\n",
        "k = 1\n",
        "\n",
        "probability_known_trigram = add_k_smooting_probability(k, vocabulary_size, trigram_probabilities[('i', 'am', 'happy')], \n",
        "                           bigram_probabilities[( 'am', 'happy')])\n",
        "\n",
        "probability_unknown_trigram = add_k_smooting_probability(k, vocabulary_size, 0, 0)\n",
        "\n",
        "print(f\"probability_known_trigram: {probability_known_trigram}\")\n",
        "print(f\"probability_unknown_trigram: {probability_unknown_trigram}\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "probability_known_trigram: 0.2\n",
            "probability_unknown_trigram: 0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cpGjPMtqUAwH",
        "outputId": "b2431e74-a803-4a2f-bf32-eb6c6f7297f3"
      },
      "source": [
        "# pre-calculated probabilities of all types of n-grams\n",
        "trigram_probabilities = {('i', 'am', 'happy'): 0.15}\n",
        "bigram_probabilities = {( 'am', 'happy'): 0.3}\n",
        "unigram_probabilities = {'happy': 0.4}\n",
        "\n",
        "# the weights come from optimization on a validation set\n",
        "lambda_1 = 0.8\n",
        "lambda_2 = 0.15\n",
        "lambda_3 = 0.05\n",
        "\n",
        "# this is the input trigram we need to estimate\n",
        "trigram = ('i', 'am', 'happy')\n",
        "\n",
        "# find the last bigram and unigram of the input\n",
        "bigram = trigram[1: 3]\n",
        "unigram = trigram[2]\n",
        "print(f\"besides the trigram {trigram} we also use bigram {bigram} and unigram ({unigram})\\n\")\n",
        "\n",
        "# in the production code, you would need to check if the probability n-gram dictionary contains the n-gram\n",
        "probability_hat_trigram = lambda_1 * trigram_probabilities[trigram] \n",
        "+ lambda_2 * bigram_probabilities[bigram]\n",
        "+ lambda_3 * unigram_probabilities[unigram]\n",
        "\n",
        "print(f\"estimated probability of the input trigram {trigram} is {probability_hat_trigram}\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "besides the trigram ('i', 'am', 'happy') we also use bigram ('am', 'happy') and unigram (happy)\n",
            "\n",
            "estimated probability of the input trigram ('i', 'am', 'happy') is 0.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcyxvUT9ULUI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}